{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Correct the headers definition (it should be a dictionary)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Step 2: Define the URL\n",
    "url = 'https://scholar.google.com/scholar?as_ylo=2023&q=inherited+retinal+diseases&hl=en&as_sdt=0,5'\n",
    "\n",
    "# Step 3: Make the GET request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Step 4: Print the response status code to ensure it worked\n",
    "print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = response.text\n",
    "doc = BeautifulSoup(page_contents,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function for the getting inforamtion of the web page\n",
    "def get_paperinfo(paper_url):\n",
    "\n",
    "  #download the page\n",
    "  response=requests.get(url,headers=headers)\n",
    "\n",
    "  # check successful response\n",
    "  if response.status_code != 200:\n",
    "    print('Status code:', response.status_code)\n",
    "    raise Exception('Failed to fetch web page ')\n",
    "\n",
    "  #parse using beautiful soup\n",
    "  paper_doc = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "  return paper_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function for the extracting information of the tags\n",
    "def get_tags(doc):\n",
    "  paper_tag = doc.select('[data-lid]')\n",
    "  cite_tag = doc.select('[title=Cite] + a')\n",
    "  link_tag = doc.find_all('h3',{\"class\" : \"gs_rt\"})\n",
    "  author_tag = doc.find_all(\"div\", {\"class\": \"gs_a\"})\n",
    "\n",
    "  return paper_tag,cite_tag,link_tag,author_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will return the title of the paper\n",
    "def get_papertitle(paper_tag):\n",
    "  \n",
    "  paper_names = []\n",
    "  \n",
    "  for tag in paper_tag:\n",
    "    paper_names.append(tag.select('h3')[0].get_text())\n",
    "\n",
    "  return paper_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will return the number of citation of the paper\n",
    "def get_citecount(cite_tag):\n",
    "  cite_count = []\n",
    "  for i in cite_tag:\n",
    "    cite = i.text\n",
    "    if i is None or cite is None:  # if paper has no citatation then consider 0\n",
    "      cite_count.append(0)\n",
    "    else:\n",
    "      tmp = re.search(r'\\d+', cite) # its handle the None type object error and re use to remove the string \" cited by \" and return only integer value\n",
    "      if tmp is None :\n",
    "        cite_count.append(0)\n",
    "      else :\n",
    "        cite_count.append(int(tmp.group()))\n",
    "\n",
    "  return cite_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the getting link information\n",
    "def get_link(link_tag):\n",
    "\n",
    "  links = []\n",
    "\n",
    "  for i in range(len(link_tag)) :\n",
    "    links.append(link_tag[i].a['href']) \n",
    "\n",
    "  return links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_year_publi_info(authors_tag):\n",
    "    years = []\n",
    "    publications = []\n",
    "    authors = []\n",
    "\n",
    "    for i in range(len(authors_tag)):\n",
    "        authortag_text = (authors_tag[i].text).split()\n",
    "        \n",
    "        # Extract year\n",
    "        year = int(re.search(r'\\d{4}', authors_tag[i].text).group()) if re.search(r'\\d{4}', authors_tag[i].text) else None\n",
    "        years.append(year)\n",
    "        \n",
    "        # Extract publication\n",
    "        publication = authortag_text[-1] if len(authortag_text) > 1 else \"Unknown\"\n",
    "        publications.append(publication)\n",
    "        \n",
    "        # Extract author name\n",
    "        author = authortag_text[0] + ' ' + re.sub(',', '', authortag_text[1]) if len(authortag_text) > 1 else \"Unknown\"\n",
    "        authors.append(author)\n",
    "\n",
    "    return years, publications, authors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doi(doc):\n",
    "    doi = None\n",
    "    doi_tag = doc.find('a', href=True, string=re.compile(r'doi', re.IGNORECASE))\n",
    "    if doi_tag:\n",
    "        doi = doi_tag['href']\n",
    "    return doi\n",
    "\n",
    "\n",
    "def get_abstract(doc):\n",
    "    abstract = None\n",
    "    abstract_tag = doc.find('div', class_='gs_rs')\n",
    "    if abstract_tag:\n",
    "        abstract = abstract_tag.get_text()\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the final repository\n",
    "paper_repos_dict = {\n",
    "    'Paper Title': [],\n",
    "    'Year': [],\n",
    "    'Author': [],\n",
    "    'Citation': [],\n",
    "    'Publication': [],\n",
    "    'Url of paper': [],\n",
    "    'DOI': [],          # New field for DOI\n",
    "    'Abstract': []      # New field for Abstract\n",
    "}\n",
    "\n",
    "# adding information in repository\n",
    "def add_in_paper_repo(papername, year, author, cite, publi, link, doi, abstract):\n",
    "    paper_repos_dict['Paper Title'].extend(papername)\n",
    "    paper_repos_dict['Year'].extend(year)\n",
    "    paper_repos_dict['Author'].extend(author)\n",
    "    paper_repos_dict['Citation'].extend(cite)\n",
    "    paper_repos_dict['Publication'].extend(publi)\n",
    "    paper_repos_dict['Url of paper'].extend(link)\n",
    "    paper_repos_dict['DOI'].extend(doi)              # Add DOI to the repository\n",
    "    paper_repos_dict['Abstract'].extend(abstract)    # Add Abstract to the repository\n",
    "\n",
    "    return pd.DataFrame(paper_repos_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 429 - Too many requests\n",
      "Error processing page starting at 0: Failed to fetch web page\n",
      "Status code: 429 - Too many requests\n",
      "Error processing page starting at 0: Failed to fetch web page\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing page starting at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Failed to fetch web page\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackoff_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     backoff_time \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Exponential backoff\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_paperinfo(url):\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'})\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    elif response.status_code == 429:\n",
    "        print(\"Status code: 429 - Too many requests\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Status code: {response.status_code} - Failed to fetch web page\")\n",
    "        return None\n",
    "\n",
    "for i in range(0, 110, 10):\n",
    "    backoff_time = 30\n",
    "    while True:\n",
    "        try:\n",
    "            # Get the URL for each page\n",
    "            url = f\"https://scholar.google.com/scholar?start={i}&q=object+detection+in+aerial+image+&hl=en&as_sdt=0,5\"\n",
    "            \n",
    "            # Fetch and parse the page\n",
    "            doc = get_paperinfo(url)\n",
    "            if doc is None:\n",
    "                print(f\"Error processing page starting at {i}: Failed to fetch web page\")\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time *= 2  # Exponential backoff\n",
    "                continue\n",
    "            \n",
    "            # Reset backoff time if successful\n",
    "            backoff_time = 30\n",
    "            \n",
    "            # Extract tags from the page\n",
    "            paper_tag, cite_tag, link_tag, author_tag = get_tags(doc)\n",
    "            \n",
    "            # Extract paper titles\n",
    "            papername = get_papertitle(paper_tag) or []\n",
    "            \n",
    "            # Extract year, author, and publication information\n",
    "            year, publication, author = get_author_year_publi_info(author_tag)\n",
    "            year = year or []\n",
    "            publication = publication or []\n",
    "            author = author or []\n",
    "            \n",
    "            # Extract DOI and abstract separately\n",
    "            doi = get_doi(doc) or \"Not Available\"\n",
    "            abstract = get_abstract(doc) or \"Not Available\"\n",
    "            \n",
    "            # Extract citation count\n",
    "            cite = get_citecount(cite_tag) or []\n",
    "            \n",
    "            # Extract paper URLs\n",
    "            link = get_link(link_tag) or []\n",
    "    \n",
    "            # Check if all lists are the same length\n",
    "            if len(papername) == len(year) == len(author) == len([doi] * len(papername)) == len([abstract] * len(papername)) == len(cite) == len(link):\n",
    "                # Add data to the repository, including DOIs and abstracts\n",
    "                final = add_in_paper_repo(papername, year, author, cite, publication, link, [doi] * len(papername), [abstract] * len(papername))\n",
    "            else:\n",
    "                print(f\"Length mismatch on page starting at {i}. Skipping this page.\")\n",
    "            \n",
    "            # Sleep to avoid rate limiting with randomized delay\n",
    "            time.sleep(random.uniform(30, 60))\n",
    "            break  # Exit the while loop and move to the next page\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page starting at {i}: {e}\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
