{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/skanda/mambaforge/envs/gpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from scispacy.linking import EntityLinker\n",
    "import scispacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import logging\n",
    "import chardet\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/skanda/mambaforge/envs/gpu/lib/python3.9/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "/mnt/data/skanda/mambaforge/envs/gpu/lib/python3.9/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/mnt/data/skanda/mambaforge/envs/gpu/lib/python3.9/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completed.\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "nlp.add_pipe(\"scispacy_linker\", last=True)\n",
    "\n",
    "print(\"Setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassemble_hyphenated_words(text):\n",
    "    return re.sub(r'(\\w+)-\\s*\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "def remove_figures_tables(text):\n",
    "    return re.sub(r'\\b(figures?|tables?)\\b', '', text)\n",
    "\n",
    "def remove_numerical_references(text):\n",
    "    return re.sub(r'\\[\\d+\\]', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_emails(text):\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    return re.sub(email_pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def remove_citations(text):\n",
    "    patterns = [\n",
    "        r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\s+et\\s+al\\.',\n",
    "        r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\s+et\\s+al\\.',\n",
    "        r'\\b[A-Z][a-z]+\\s+et\\s+al\\.',\n",
    "        r'\\(.*?et al\\..*?\\d{4}.*?\\)',\n",
    "        r'\\[.*?\\]',\n",
    "        r'\\(\\d{4}[a-z]?(?:,\\s*\\d{4}[a-z]?)*\\)',\n",
    "        r'^.*?\\d{4};.*?:\\s*\\d+.*?$',\n",
    "        r'^.*?\\d{4};.*?:\\s*\\d+.*?$',  # Matches journal info like \"2024;258: 119– 129\"\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def remove_headers(text):\n",
    "    # Remove lines that are all uppercase and end with a colon\n",
    "    text = re.sub(r'^[A-Z\\s]+:$', '', text, flags=re.MULTILINE)\n",
    "    # Remove lines that start with bullet points\n",
    "    text = re.sub(r'^\\s*•.*$', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_metadata(text):\n",
    "    # Remove headers, copyright info, DOI, received/accepted dates\n",
    "    patterns = [\n",
    "        r'^.*?©Copyright.*$',\n",
    "        r'^DOI:.*$',\n",
    "        r'^Received:.*$',\n",
    "        r'^Accepted:.*$',\n",
    "        r'^Address for Correspondence:.*$',\n",
    "        r'^E-mail:.*$',\n",
    "        r'^ORCID-ID:.*$',\n",
    "        r'^\\s*\\d+\\s*$',  # Page numbers\n",
    "        r'^.*?ORCID:.*$',\n",
    "        r'^Cite this article as:.*$',\n",
    "        r'\\[\\s*[^\\w\\s]*\\s*\\]'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_institution_names(text):\n",
    "    # Remove institution names (this is a simplified approach and may need refinement)\n",
    "    pattern = r'\\*+[A-Z][A-Za-z\\s,]+(University|Institute|Hospital|Clinic|Department|Faculty)[^\\n]*'\n",
    "    return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_copyright_info(text):\n",
    "    patterns = [\n",
    "        r'^©.*$',\n",
    "        r'Copyright.*$',\n",
    "        r'This is an open access article.*$',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def remove_doi_and_journal_info(text):\n",
    "    patterns = [\n",
    "        r'DOI:.*$',\n",
    "        r'^.*?\\d{4};\\d+:\\d+–\\d+',  # Matches journal info like \"2024;258: 119– 129\"\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_references(text):\n",
    "    # Remove lines that look like references\n",
    "    text = re.sub(r'^.*?\\d{4};.*?:\\s*\\d+.*?$', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_artifacts(text):\n",
    "    # Remove license and DOI information\n",
    "    text = re.sub(r'BY license \\(.*?\\)\\..*?commons\\.org/licenses/by/\\d\\.\\d/\\s*\\)\\.', '', text)\n",
    "    text = re.sub(r'://doi\\.org/\\d+\\.\\d+/[^\\s]+', '', text)\n",
    "    text = re.sub(r'://creativecommons\\.org/licenses/by/\\d\\.\\d/', '', text)\n",
    "    text = re.sub(r'://creativecommons\\.org/licenses/by-\\w+/\\d\\.\\d/', '', text)\n",
    "    \n",
    "    \n",
    "    # Remove unnecessary symbols\n",
    "    text = re.sub(r'[⁎\\]]', '', text)\n",
    "    \n",
    "    # Remove empty parentheses and brackets\n",
    "    text = re.sub(r'\\(\\s*\\)|\\[\\s*\\]', '', text)\n",
    "    \n",
    "    # Remove isolated semicolons\n",
    "    text = re.sub(r'\\s*;\\s*', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sentence_boundaries(text):\n",
    "    \"\"\"\n",
    "    Detect sentence boundaries using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    spacy_sentences = [sent.text for sent in doc.sents]\n",
    "    return ' '.join(spacy_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, tokenize=False):\n",
    "    try:\n",
    "\n",
    "        print(f\"Starting preprocessing. Initial text length: {len(text)}\")\n",
    "\n",
    "        # Step 1: Initial text cleaning\n",
    "        text = reassemble_hyphenated_words(text)\n",
    "        print(f\"After reassemble_hyphenated_words: {len(text)} chars\")\n",
    "\n",
    "        text = remove_figures_tables(text)\n",
    "        print(f\"After remove_figures_tables: {len(text)} chars\")\n",
    "\n",
    "        text = remove_citations(text)\n",
    "        print(f\"After remove_citations: {len(text)} chars\")\n",
    "\n",
    "        text = remove_urls(text)\n",
    "        print(f\"After remove_urls: {len(text)} chars\")\n",
    "\n",
    "        text = remove_emails(text)\n",
    "        print(f\"After remove_emails: {len(text)} chars\")\n",
    "\n",
    "        text = remove_numerical_references(text)\n",
    "        print(f\"After remove_numerical_references: {len(text)} chars\")\n",
    "\n",
    "        text = remove_headers(text)\n",
    "        print(f\"After remove_headers: {len(text)} chars\")\n",
    "\n",
    "        text = remove_references(text)\n",
    "        print(f\"After remove_references: {len(text)} chars\")\n",
    "\n",
    "        text = remove_metadata(text)\n",
    "        print(f\"After remove_metadata: {len(text)} chars\")\n",
    "\n",
    "        text = remove_institution_names(text)\n",
    "        print(f\"After remove_institution_names: {len(text)} chars\")\n",
    "        \n",
    "        text = remove_copyright_info(text)\n",
    "        print(f\"After remove_copyright_info: {len(text)} chars\")\n",
    "\n",
    "        text = remove_doi_and_journal_info(text)\n",
    "        print(f\"After remove_doi_and_journal_info: {len(text)} chars\")\n",
    "\n",
    "        text = remove_artifacts(text)\n",
    "        print(f\"After remove_artifacts: {len(text)} chars\")\n",
    "\n",
    "        # Step 2: Sentence boundary detection\n",
    "        text = detect_sentence_boundaries(text)\n",
    "        print(f\"After detect_sentence_boundaries: {len(text)} chars\")\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        print(f\"After removing extra whitespace: {len(text)} chars\")\n",
    "    \n",
    "        if tokenize:\n",
    "            tokens = tokenize_text(text)\n",
    "            print(f\"After tokenization: {len(tokens)} tokens\")\n",
    "            return tokens\n",
    "        else:\n",
    "            print(\"Text preprocessing complete.\")\n",
    "            return text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in preprocess_text: {e}\")\n",
    "        print(f\"Error in preprocess_text: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Count the number of words in the given text.\"\"\"\n",
    "    return len(re.findall(r'\\w+', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document(text):\n",
    "    # Process the document with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Split the document into paragraphs\n",
    "    paragraphs = [para.text for para in doc.sents]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(para, target):\n",
    "    # Convert the paragraph and target text to spaCy tokens\n",
    "    para_doc = nlp(para)\n",
    "    target_doc = nlp(target)\n",
    "    # Calculate the cosine similarity between the embeddings\n",
    "    similarity = para_doc.similarity(target_doc)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_abstract_section(paragraphs, start_targets=[\"Abstract\", \"Background\", \"Methods\", \"Results\", \"Conclusion\"], end_targets=[\"Keywords\", \"Introduction\"], threshold=0.9):\n",
    "    start_index = None\n",
    "    end_index = None\n",
    "    min_abstract_length = 120  # Minimum number of words in the abstract\n",
    "    max_start_search = 3  # Limit the number of paragraphs to consider if no heading is found\n",
    "\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        # Check for any of the start targets\n",
    "        for start_target in start_targets:\n",
    "            start_similarity = calculate_similarity(para, start_target)\n",
    "            if start_similarity > threshold and start_index is None:\n",
    "                start_index = i\n",
    "                print(f\"Start of abstract detected at paragraph {i}: {para[:30]} with start target '{start_target}'...\")\n",
    "                break  # Found the start, no need to check other start targets\n",
    "\n",
    "        # If no heading is found within the first few paragraphs, assume the first paragraph is the start\n",
    "        if start_index is None and i < max_start_search:\n",
    "            start_index = 0\n",
    "            print(f\"No start heading detected, assuming paragraph 0 as the start of the abstract.\")\n",
    "            break\n",
    "\n",
    "        # Once start is detected, look for the end target\n",
    "        if start_index is not None and i >= start_index:\n",
    "            for end_target in end_targets:\n",
    "                end_similarity = calculate_similarity(para, end_target)\n",
    "                if end_similarity > threshold:\n",
    "                    end_index = i\n",
    "                    print(f\"End of abstract detected at paragraph {i}: {para[:30]} with end target '{end_target}'...\")\n",
    "                    break  # Found the end, no need to check other end targets\n",
    "            if end_index is not None:\n",
    "                break\n",
    "\n",
    "    # Extract and validate the abstract\n",
    "    if start_index is not None and end_index is not None:\n",
    "        abstract = \"\\n\".join(paragraphs[start_index:end_index])\n",
    "        # Check if the abstract is too short\n",
    "        if count_words(abstract) < min_abstract_length:\n",
    "            print(\"Detected abstract is too short. Rechecking...\")\n",
    "            abstract = \"\"  # Discard or handle short abstracts\n",
    "    elif start_index is not None:\n",
    "        # If no end target is found, but start is identified, take a few paragraphs as abstract\n",
    "        abstract = \"\\n\".join(paragraphs[start_index:start_index + max_start_search])\n",
    "        print(f\"Taking first {max_start_search} paragraphs as abstract since no end target found.\")\n",
    "    else:\n",
    "        abstract = \"\"  # No abstract found\n",
    "    return abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract_from_document(text):\n",
    "    # Preprocess the text\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    # Split the document into paragraphs\n",
    "    paragraphs = split_document(cleaned_text)\n",
    "    # Detect and extract the abstract section\n",
    "    abstract = detect_abstract_section(paragraphs)\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(input_folder_path: str, output_folder_path: str, batch_size: int = 10, tokenize=False) -> None:\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "    \n",
    "    files = [f for f in os.listdir(input_folder_path) if f.endswith('.txt')]\n",
    "    total_files = len(files)\n",
    "    \n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch = files[i:i+batch_size]\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1} of {(total_files-1)//batch_size + 1}\")\n",
    "        \n",
    "        for filename in tqdm(batch, desc=f\"Batch {i//batch_size + 1}\"):\n",
    "            input_file_path = os.path.join(input_folder_path, filename)\n",
    "            output_file_path = os.path.join(output_folder_path, f\"preprocessed_{filename}\")\n",
    "            abstract_output_path = os.path.join(output_folder_path, f\"abstract_{filename}\")\n",
    "            \n",
    "            print(f\"\\nProcessing file: {filename}\")\n",
    "            \n",
    "            try:\n",
    "                # Detect file encoding\n",
    "                with open(input_file_path, 'rb') as file:\n",
    "                    raw_data = file.read()\n",
    "                    result = chardet.detect(raw_data)\n",
    "                    detected_encoding = result['encoding']\n",
    "                    confidence = result['confidence']\n",
    "                \n",
    "                print(f\"Detected encoding: {detected_encoding} (confidence: {confidence:.2f})\")\n",
    "                \n",
    "                # Try reading with detected encoding\n",
    "                try:\n",
    "                    with open(input_file_path, 'r', encoding=detected_encoding) as file:\n",
    "                        original_text = file.read()\n",
    "                    print(f\"Successfully read file with {detected_encoding} encoding.\")\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"Failed to read with {detected_encoding}. Trying UTF-8...\")\n",
    "                    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                        original_text = file.read()\n",
    "                    print(\"Successfully read file with UTF-8 encoding.\")\n",
    "                \n",
    "                original_word_count = count_words(original_text)\n",
    "                print(f\"Original word count: {original_word_count}\")\n",
    "                \n",
    "                cleaned_text = preprocess_text(original_text, tokenize=tokenize)\n",
    "                \n",
    "                # Extract the abstract\n",
    "                abstract = extract_abstract_from_document(original_text)\n",
    "                print(f\"Extracted abstract: {len(abstract)} characters\")\n",
    "\n",
    "                if tokenize:\n",
    "                    cleaned_word_count = len(cleaned_text)  # cleaned_text is now a list of tokens\n",
    "                    # Save tokens, one per line\n",
    "                    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                        file.write('\\n'.join(cleaned_text))\n",
    "                else:\n",
    "                    cleaned_word_count = count_words(cleaned_text)\n",
    "                    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                        file.write(cleaned_text)\n",
    "                \n",
    "                # Save the abstract to a separate file\n",
    "                with open(abstract_output_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(abstract)\n",
    "                \n",
    "                print(f\"Abstract saved to: {abstract_output_path}\")\n",
    "                print(f\"Cleaned text saved to: {output_file_path}\")\n",
    "\n",
    "                # Calculate and print the percentage of cleaned text\n",
    "                if original_word_count > 0:\n",
    "                    percentage_retained = (cleaned_word_count / original_word_count) * 100\n",
    "                    print(f\"Percentage of words retained after cleaning: {percentage_retained:.2f}%\")\n",
    "                else:\n",
    "                    print(\"Original text contains no words; cannot calculate percentage.\")\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {input_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {str(e)}\")\n",
    "            \n",
    "            print(\"=\" * 100)\n",
    "        \n",
    "        print(f\"Completed processing batch {i//batch_size + 1}\")\n",
    "    \n",
    "    print(\"All batches processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 13287_2023_Article_3526.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 7885\n",
      "Starting preprocessing. Initial text length: 47767\n",
      "After reassemble_hyphenated_words: 47713 chars\n",
      "After remove_figures_tables: 47713 chars\n",
      "After remove_citations: 43117 chars\n",
      "After remove_urls: 42951 chars\n",
      "After remove_emails: 42929 chars\n",
      "After remove_numerical_references: 42929 chars\n",
      "After remove_headers: 42920 chars\n",
      "After remove_references: 42920 chars\n",
      "After remove_metadata: 42866 chars\n",
      "After remove_institution_names: 42866 chars\n",
      "After remove_copyright_info: 42696 chars\n",
      "After remove_doi_and_journal_info: 42696 chars\n",
      "After remove_artifacts: 41770 chars\n",
      "After detect_sentence_boundaries: 41773 chars\n",
      "After removing extra whitespace: 41773 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 47767\n",
      "After reassemble_hyphenated_words: 47713 chars\n",
      "After remove_figures_tables: 47713 chars\n",
      "After remove_citations: 43117 chars\n",
      "After remove_urls: 42951 chars\n",
      "After remove_emails: 42929 chars\n",
      "After remove_numerical_references: 42929 chars\n",
      "After remove_headers: 42920 chars\n",
      "After remove_references: 42920 chars\n",
      "After remove_metadata: 42866 chars\n",
      "After remove_institution_names: 42866 chars\n",
      "After remove_copyright_info: 42696 chars\n",
      "After remove_doi_and_journal_info: 42696 chars\n",
      "After remove_artifacts: 41770 chars\n",
      "After detect_sentence_boundaries: 41773 chars\n",
      "After removing extra whitespace: 41773 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  10%|█         | 1/10 [00:04<00:42,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 626 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_13287_2023_Article_3526.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_13287_2023_Article_3526.txt\n",
      "Percentage of words retained after cleaning: 86.79%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: EMMM-14-e15941-compressed.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 15087\n",
      "Starting preprocessing. Initial text length: 93780\n",
      "After reassemble_hyphenated_words: 93624 chars\n",
      "After remove_figures_tables: 93606 chars\n",
      "After remove_citations: 91854 chars\n",
      "After remove_urls: 91709 chars\n",
      "After remove_emails: 91642 chars\n",
      "After remove_numerical_references: 91642 chars\n",
      "After remove_headers: 91642 chars\n",
      "After remove_references: 91642 chars\n",
      "After remove_metadata: 91641 chars\n",
      "After remove_institution_names: 91641 chars\n",
      "After remove_copyright_info: 91601 chars\n",
      "After remove_doi_and_journal_info: 91601 chars\n",
      "After remove_artifacts: 91370 chars\n",
      "After detect_sentence_boundaries: 91382 chars\n",
      "After removing extra whitespace: 91382 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 93780\n",
      "After reassemble_hyphenated_words: 93624 chars\n",
      "After remove_figures_tables: 93606 chars\n",
      "After remove_citations: 91854 chars\n",
      "After remove_urls: 91709 chars\n",
      "After remove_emails: 91642 chars\n",
      "After remove_numerical_references: 91642 chars\n",
      "After remove_headers: 91642 chars\n",
      "After remove_references: 91642 chars\n",
      "After remove_metadata: 91641 chars\n",
      "After remove_institution_names: 91641 chars\n",
      "After remove_copyright_info: 91601 chars\n",
      "After remove_doi_and_journal_info: 91601 chars\n",
      "After remove_artifacts: 91370 chars\n",
      "After detect_sentence_boundaries: 91382 chars\n",
      "After removing extra whitespace: 91382 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  20%|██        | 2/10 [00:12<00:53,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 968 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_EMMM-14-e15941-compressed.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_EMMM-14-e15941-compressed.txt\n",
      "Percentage of words retained after cleaning: 97.29%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: 12886_2023_Article_2772-compressed.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 5451\n",
      "Starting preprocessing. Initial text length: 32368\n",
      "After reassemble_hyphenated_words: 32344 chars\n",
      "After remove_figures_tables: 32344 chars\n",
      "After remove_citations: 30858 chars\n",
      "After remove_urls: 30795 chars\n",
      "After remove_emails: 30782 chars\n",
      "After remove_numerical_references: 30782 chars\n",
      "After remove_headers: 30772 chars\n",
      "After remove_references: 30772 chars\n",
      "After remove_metadata: 30721 chars\n",
      "After remove_institution_names: 30721 chars\n",
      "After remove_copyright_info: 30551 chars\n",
      "After remove_doi_and_journal_info: 30551 chars\n",
      "After remove_artifacts: 29872 chars\n",
      "After detect_sentence_boundaries: 29875 chars\n",
      "After removing extra whitespace: 29875 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 32368\n",
      "After reassemble_hyphenated_words: 32344 chars\n",
      "After remove_figures_tables: 32344 chars\n",
      "After remove_citations: 30858 chars\n",
      "After remove_urls: 30795 chars\n",
      "After remove_emails: 30782 chars\n",
      "After remove_numerical_references: 30782 chars\n",
      "After remove_headers: 30772 chars\n",
      "After remove_references: 30772 chars\n",
      "After remove_metadata: 30721 chars\n",
      "After remove_institution_names: 30721 chars\n",
      "After remove_copyright_info: 30551 chars\n",
      "After remove_doi_and_journal_info: 30551 chars\n",
      "After remove_artifacts: 29872 chars\n",
      "After detect_sentence_boundaries: 29875 chars\n",
      "After removing extra whitespace: 29875 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  30%|███       | 3/10 [00:15<00:34,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 736 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_12886_2023_Article_2772-compressed.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_12886_2023_Article_2772-compressed.txt\n",
      "Percentage of words retained after cleaning: 93.45%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: 13023_2021_Article_2145.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 6776\n",
      "Starting preprocessing. Initial text length: 44620\n",
      "After reassemble_hyphenated_words: 44570 chars\n",
      "After remove_figures_tables: 44570 chars\n",
      "After remove_citations: 41943 chars\n",
      "After remove_urls: 41771 chars\n",
      "After remove_emails: 41753 chars\n",
      "After remove_numerical_references: 41753 chars\n",
      "After remove_headers: 41743 chars\n",
      "After remove_references: 41743 chars\n",
      "After remove_metadata: 41692 chars\n",
      "After remove_institution_names: 41692 chars\n",
      "After remove_copyright_info: 41522 chars\n",
      "After remove_doi_and_journal_info: 41522 chars\n",
      "After remove_artifacts: 40779 chars\n",
      "After detect_sentence_boundaries: 40779 chars\n",
      "After removing extra whitespace: 40779 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 44620\n",
      "After reassemble_hyphenated_words: 44570 chars\n",
      "After remove_figures_tables: 44570 chars\n",
      "After remove_citations: 41943 chars\n",
      "After remove_urls: 41771 chars\n",
      "After remove_emails: 41753 chars\n",
      "After remove_numerical_references: 41753 chars\n",
      "After remove_headers: 41743 chars\n",
      "After remove_references: 41743 chars\n",
      "After remove_metadata: 41692 chars\n",
      "After remove_institution_names: 41692 chars\n",
      "After remove_copyright_info: 41522 chars\n",
      "After remove_doi_and_journal_info: 41522 chars\n",
      "After remove_artifacts: 40779 chars\n",
      "After detect_sentence_boundaries: 40779 chars\n",
      "After removing extra whitespace: 40779 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  40%|████      | 4/10 [00:19<00:26,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 620 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_13023_2021_Article_2145.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_13023_2021_Article_2145.txt\n",
      "Percentage of words retained after cleaning: 91.12%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: 41433_2022_Article_2262.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 3707\n",
      "Starting preprocessing. Initial text length: 22967\n",
      "After reassemble_hyphenated_words: 22945 chars\n",
      "After remove_figures_tables: 22945 chars\n",
      "After remove_citations: 21575 chars\n",
      "After remove_urls: 21461 chars\n",
      "After remove_emails: 21439 chars\n",
      "After remove_numerical_references: 21439 chars\n",
      "After remove_headers: 21439 chars\n",
      "After remove_references: 21439 chars\n",
      "After remove_metadata: 21351 chars\n",
      "After remove_institution_names: 21351 chars\n",
      "After remove_copyright_info: 21265 chars\n",
      "After remove_doi_and_journal_info: 21265 chars\n",
      "After remove_artifacts: 21223 chars\n",
      "After detect_sentence_boundaries: 21231 chars\n",
      "After removing extra whitespace: 21231 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 22967\n",
      "After reassemble_hyphenated_words: 22945 chars\n",
      "After remove_figures_tables: 22945 chars\n",
      "After remove_citations: 21575 chars\n",
      "After remove_urls: 21461 chars\n",
      "After remove_emails: 21439 chars\n",
      "After remove_numerical_references: 21439 chars\n",
      "After remove_headers: 21439 chars\n",
      "After remove_references: 21439 chars\n",
      "After remove_metadata: 21351 chars\n",
      "After remove_institution_names: 21351 chars\n",
      "After remove_copyright_info: 21265 chars\n",
      "After remove_doi_and_journal_info: 21265 chars\n",
      "After remove_artifacts: 21223 chars\n",
      "After detect_sentence_boundaries: 21231 chars\n",
      "After removing extra whitespace: 21231 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  50%|█████     | 5/10 [00:20<00:17,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 508 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_41433_2022_Article_2262.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_41433_2022_Article_2262.txt\n",
      "Percentage of words retained after cleaning: 91.99%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: emss-80329.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 15309\n",
      "Starting preprocessing. Initial text length: 100603\n",
      "After reassemble_hyphenated_words: 100554 chars\n",
      "After remove_figures_tables: 100554 chars\n",
      "After remove_citations: 87764 chars\n",
      "After remove_urls: 87764 chars\n",
      "After remove_emails: 87745 chars\n",
      "After remove_numerical_references: 87745 chars\n",
      "After remove_headers: 87745 chars\n",
      "After remove_references: 87745 chars\n",
      "After remove_metadata: 87745 chars\n",
      "After remove_institution_names: 87745 chars\n",
      "After remove_copyright_info: 87745 chars\n",
      "After remove_doi_and_journal_info: 87745 chars\n",
      "After remove_artifacts: 86482 chars\n",
      "After detect_sentence_boundaries: 86483 chars\n",
      "After removing extra whitespace: 86483 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 100603\n",
      "After reassemble_hyphenated_words: 100554 chars\n",
      "After remove_figures_tables: 100554 chars\n",
      "After remove_citations: 87764 chars\n",
      "After remove_urls: 87764 chars\n",
      "After remove_emails: 87745 chars\n",
      "After remove_numerical_references: 87745 chars\n",
      "After remove_headers: 87745 chars\n",
      "After remove_references: 87745 chars\n",
      "After remove_metadata: 87745 chars\n",
      "After remove_institution_names: 87745 chars\n",
      "After remove_copyright_info: 87745 chars\n",
      "After remove_doi_and_journal_info: 87745 chars\n",
      "After remove_artifacts: 86482 chars\n",
      "After detect_sentence_boundaries: 86483 chars\n",
      "After removing extra whitespace: 86483 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  60%|██████    | 6/10 [00:29<00:20,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 854 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_emss-80329.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_emss-80329.txt\n",
      "Percentage of words retained after cleaning: 85.66%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: 41436_2020_Article_759.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 6909\n",
      "Starting preprocessing. Initial text length: 44036\n",
      "After reassemble_hyphenated_words: 43988 chars\n",
      "After remove_figures_tables: 43988 chars\n",
      "After remove_citations: 41017 chars\n",
      "After remove_urls: 40797 chars\n",
      "After remove_emails: 40764 chars\n",
      "After remove_numerical_references: 40764 chars\n",
      "After remove_headers: 40764 chars\n",
      "After remove_references: 40764 chars\n",
      "After remove_metadata: 40705 chars\n",
      "After remove_institution_names: 40705 chars\n",
      "After remove_copyright_info: 40604 chars\n",
      "After remove_doi_and_journal_info: 40604 chars\n",
      "After remove_artifacts: 40506 chars\n",
      "After detect_sentence_boundaries: 40518 chars\n",
      "After removing extra whitespace: 40518 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 44036\n",
      "After reassemble_hyphenated_words: 43988 chars\n",
      "After remove_figures_tables: 43988 chars\n",
      "After remove_citations: 41017 chars\n",
      "After remove_urls: 40797 chars\n",
      "After remove_emails: 40764 chars\n",
      "After remove_numerical_references: 40764 chars\n",
      "After remove_headers: 40764 chars\n",
      "After remove_references: 40764 chars\n",
      "After remove_metadata: 40705 chars\n",
      "After remove_institution_names: 40705 chars\n",
      "After remove_copyright_info: 40604 chars\n",
      "After remove_doi_and_journal_info: 40604 chars\n",
      "After remove_artifacts: 40506 chars\n",
      "After detect_sentence_boundaries: 40518 chars\n",
      "After removing extra whitespace: 40518 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  70%|███████   | 7/10 [00:36<00:17,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 851 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_41436_2020_Article_759.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_41436_2020_Article_759.txt\n",
      "Percentage of words retained after cleaning: 91.61%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: 13023_2023_Article_2798.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 9875\n",
      "Starting preprocessing. Initial text length: 65921\n",
      "After reassemble_hyphenated_words: 65855 chars\n",
      "After remove_figures_tables: 65842 chars\n",
      "After remove_citations: 60900 chars\n",
      "After remove_urls: 60793 chars\n",
      "After remove_emails: 60763 chars\n",
      "After remove_numerical_references: 60763 chars\n",
      "After remove_headers: 60228 chars\n",
      "After remove_references: 60228 chars\n",
      "After remove_metadata: 60179 chars\n",
      "After remove_institution_names: 60179 chars\n",
      "After remove_copyright_info: 60008 chars\n",
      "After remove_doi_and_journal_info: 60008 chars\n",
      "After remove_artifacts: 58913 chars\n",
      "After detect_sentence_boundaries: 58918 chars\n",
      "After removing extra whitespace: 58918 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 65921\n",
      "After reassemble_hyphenated_words: 65855 chars\n",
      "After remove_figures_tables: 65842 chars\n",
      "After remove_citations: 60900 chars\n",
      "After remove_urls: 60793 chars\n",
      "After remove_emails: 60763 chars\n",
      "After remove_numerical_references: 60763 chars\n",
      "After remove_headers: 60228 chars\n",
      "After remove_references: 60228 chars\n",
      "After remove_metadata: 60179 chars\n",
      "After remove_institution_names: 60179 chars\n",
      "After remove_copyright_info: 60008 chars\n",
      "After remove_doi_and_journal_info: 60008 chars\n",
      "After remove_artifacts: 58913 chars\n",
      "After detect_sentence_boundaries: 58918 chars\n",
      "After removing extra whitespace: 58918 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  80%|████████  | 8/10 [00:41<00:11,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 736 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_13023_2023_Article_2798.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_13023_2023_Article_2798.txt\n",
      "Percentage of words retained after cleaning: 88.61%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: diagnostics-13-00850.txt\n",
      "Detected encoding: utf-8 (confidence: 0.99)\n",
      "Successfully read file with utf-8 encoding.\n",
      "Original word count: 6913\n",
      "Starting preprocessing. Initial text length: 41753\n",
      "After reassemble_hyphenated_words: 41729 chars\n",
      "After remove_figures_tables: 41729 chars\n",
      "After remove_citations: 40843 chars\n",
      "After remove_urls: 40629 chars\n",
      "After remove_emails: 40595 chars\n",
      "After remove_numerical_references: 40595 chars\n",
      "After remove_headers: 40595 chars\n",
      "After remove_references: 40595 chars\n",
      "After remove_metadata: 40543 chars\n",
      "After remove_institution_names: 40543 chars\n",
      "After remove_copyright_info: 40510 chars\n",
      "After remove_doi_and_journal_info: 40510 chars\n",
      "After remove_artifacts: 39932 chars\n",
      "After detect_sentence_boundaries: 39932 chars\n",
      "After removing extra whitespace: 39932 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 41753\n",
      "After reassemble_hyphenated_words: 41729 chars\n",
      "After remove_figures_tables: 41729 chars\n",
      "After remove_citations: 40843 chars\n",
      "After remove_urls: 40629 chars\n",
      "After remove_emails: 40595 chars\n",
      "After remove_numerical_references: 40595 chars\n",
      "After remove_headers: 40595 chars\n",
      "After remove_references: 40595 chars\n",
      "After remove_metadata: 40543 chars\n",
      "After remove_institution_names: 40543 chars\n",
      "After remove_copyright_info: 40510 chars\n",
      "After remove_doi_and_journal_info: 40510 chars\n",
      "After remove_artifacts: 39932 chars\n",
      "After detect_sentence_boundaries: 39932 chars\n",
      "After removing extra whitespace: 39932 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  90%|█████████ | 9/10 [00:45<00:05,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 311 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_diagnostics-13-00850.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_diagnostics-13-00850.txt\n",
      "Percentage of words retained after cleaning: 97.08%\n",
      "====================================================================================================\n",
      "\n",
      "Processing file: diagnostics-13-02413-compressed.txt\n",
      "Detected encoding: MacRoman (confidence: 0.68)\n",
      "Successfully read file with MacRoman encoding.\n",
      "Original word count: 18516\n",
      "Starting preprocessing. Initial text length: 116515\n",
      "After reassemble_hyphenated_words: 116351 chars\n",
      "After remove_figures_tables: 116351 chars\n",
      "After remove_citations: 113399 chars\n",
      "After remove_urls: 113265 chars\n",
      "After remove_emails: 113098 chars\n",
      "After remove_numerical_references: 113098 chars\n",
      "After remove_headers: 113098 chars\n",
      "After remove_references: 113098 chars\n",
      "After remove_metadata: 113055 chars\n",
      "After remove_institution_names: 113055 chars\n",
      "After remove_copyright_info: 112970 chars\n",
      "After remove_doi_and_journal_info: 112970 chars\n",
      "After remove_artifacts: 111711 chars\n",
      "After detect_sentence_boundaries: 111711 chars\n",
      "After removing extra whitespace: 111711 chars\n",
      "Text preprocessing complete.\n",
      "Starting preprocessing. Initial text length: 116515\n",
      "After reassemble_hyphenated_words: 116351 chars\n",
      "After remove_figures_tables: 116351 chars\n",
      "After remove_citations: 113399 chars\n",
      "After remove_urls: 113265 chars\n",
      "After remove_emails: 113098 chars\n",
      "After remove_numerical_references: 113098 chars\n",
      "After remove_headers: 113098 chars\n",
      "After remove_references: 113098 chars\n",
      "After remove_metadata: 113055 chars\n",
      "After remove_institution_names: 113055 chars\n",
      "After remove_copyright_info: 112970 chars\n",
      "After remove_doi_and_journal_info: 112970 chars\n",
      "After remove_artifacts: 111711 chars\n",
      "After detect_sentence_boundaries: 111711 chars\n",
      "After removing extra whitespace: 111711 chars\n",
      "Text preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 10/10 [00:55<00:00,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No start heading detected, assuming paragraph 0 as the start of the abstract.\n",
      "Taking first 3 paragraphs as abstract since no end target found.\n",
      "Extracted abstract: 581 characters\n",
      "Abstract saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/abstract_diagnostics-13-02413-compressed.txt\n",
      "Cleaned text saved to: /mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed/preprocessed_diagnostics-13-02413-compressed.txt\n",
      "Percentage of words retained after cleaning: 96.62%\n",
      "====================================================================================================\n",
      "Completed processing batch 1\n",
      "All batches processed.\n",
      "Preprocessing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"/mnt/data/skanda/MSc_IRD_LLM/data/data_text_sample\"\n",
    "    output_folder = \"/mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed\"\n",
    "    abstract_folder = \"/mnt/data/skanda/MSc_IRD_LLM/data/abstracts\"\n",
    "    process_files(input_folder, output_folder, batch_size=10, tokenize=False)\n",
    "    print(\"Preprocessing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
