{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from scispacy.linking import EntityLinker\n",
    "import scispacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import logging\n",
    "import chardet\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/skanda/mambaforge/envs/gpu/lib/python3.9/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/mnt/data/skanda/mambaforge/envs/gpu/lib/python3.9/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completed.\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "nlp.add_pipe(\"scispacy_linker\", last=True)\n",
    "\n",
    "print(\"Setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_with_links(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        # Get the linked concepts for each entity\n",
    "        for umls_ent in ent._.kb_ents:\n",
    "            concept_id, score = umls_ent\n",
    "            entities.append({\n",
    "                \"text\": ent.text,\n",
    "                \"label\": ent.label_,\n",
    "                \"concept_id\": concept_id,\n",
    "                \"score\": score\n",
    "            })\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassemble_hyphenated_words\n",
    "def reassemble_hyphenated_words(text):\n",
    "    return re.sub(r'(\\w+)-\\s*\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emails(text):\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    return re.sub(email_pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def remove_figures_tables(text):\n",
    "    return re.sub(r'\\b(figures?|tables?)\\b', '', text)\n",
    "\n",
    "def remove_numerical_references(text):\n",
    "    return re.sub(r'\\[\\d+\\]', '', text)\n",
    "\n",
    "def remove_citations(text):\n",
    "    patterns = [\n",
    "        r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\s+et\\s+al\\.',\n",
    "        r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\s+et\\s+al\\.',\n",
    "        r'\\b[A-Z][a-z]+\\s+et\\s+al\\.',\n",
    "        r'\\(.*?et al\\..*?\\d{4}.*?\\)',\n",
    "        r'\\[.*?\\]',\n",
    "        r'\\(\\d{4}[a-z]?(?:,\\s*\\d{4}[a-z]?)*\\)',\n",
    "        r'^.*?\\d{4};.*?:\\s*\\d+.*?$',\n",
    "        r'^.*?\\d{4};.*?:\\s*\\d+.*?$',  # Matches journal info like \"2024;258: 119– 129\"\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def remove_headers(text):\n",
    "    # Remove lines that are all uppercase and end with a colon\n",
    "    text = re.sub(r'^[A-Z\\s]+:$', '', text, flags=re.MULTILINE)\n",
    "    # Remove lines that start with bullet points\n",
    "    text = re.sub(r'^\\s*•.*$', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_metadata(text):\n",
    "    # Remove headers, copyright info, DOI, received/accepted dates\n",
    "    patterns = [\n",
    "        r'^.*?©Copyright.*$',\n",
    "        r'^DOI:.*$',\n",
    "        r'^Received:.*$',\n",
    "        r'^Accepted:.*$',\n",
    "        r'^Address for Correspondence:.*$',\n",
    "        r'^E-mail:.*$',\n",
    "        r'^ORCID-ID:.*$',\n",
    "        r'^\\s*\\d+\\s*$',  # Page numbers\n",
    "        r'^.*?ORCID:.*$',\n",
    "        r'^Cite this article as:.*$',\n",
    "        r'\\[\\s*[^\\w\\s]*\\s*\\]'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "# def remove_institution_names(text):\n",
    "#     # Remove institution names (this is a simplified approach and may need refinement)\n",
    "#     pattern = r'\\*+[A-Z][A-Za-z\\s,]+(University|Institute|Hospital|Department|Faculty)[^\\n]*'\n",
    "#     return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "def handle_special_characters(text):\n",
    "    \"\"\"Handle special characters and Unicode normalization.\"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    char_map = {\n",
    "        '´': \"'\", '‘': \"'\", '’': \"'\", '“': '\"', '”': '\"', '–': '-', '—': '-', '…': '...'\n",
    "    }\n",
    "    for char, replacement in char_map.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    return text\n",
    "\n",
    "def remove_copyright_info(text):\n",
    "    patterns = [\n",
    "        r'^©.*$',\n",
    "        r'Copyright.*$',\n",
    "        r'This is an open access article.*$',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def remove_doi_and_journal_info(text):\n",
    "    patterns = [\n",
    "        r'DOI:.*$',\n",
    "        r'^.*?\\d{4};\\d+:\\d+–\\d+',  # Matches journal info like \"2024;258: 119– 129\"\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_artifacts(text):\n",
    "    # Remove license and DOI information\n",
    "    text = re.sub(r'BY license \\(.*?\\)\\..*?commons\\.org/licenses/by/\\d\\.\\d/\\s*\\)\\.', '', text)\n",
    "    text = re.sub(r'://doi\\.org/\\d+\\.\\d+/[^\\s]+', '', text )\n",
    "    text = re.sub(r'://creativecommons\\.org/licenses/by/\\d\\.\\d/', '', text)\n",
    "    text = re.sub(r'://creativecommons\\.org/licenses/by-\\w+/\\d\\.\\d/', '', text)\n",
    "    \n",
    "    \n",
    "    # Remove unnecessary symbols\n",
    "    text = re.sub(r'[⁎\\]]', '', text)\n",
    "    \n",
    "    # Remove empty parentheses and brackets\n",
    "    text = re.sub(r'\\(\\s*\\)|\\[\\s*\\]', '', text)\n",
    "    \n",
    "    # Remove isolated semicolons\n",
    "    text = re.sub(r'\\s*;\\s*', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_references(text):\n",
    "    \"\"\"\n",
    "    Remove the references section from the text. This function looks for the word 'References'\n",
    "    followed by '1.' and removes everything from that point onward.\n",
    "    \"\"\"\n",
    "    # Pattern to match 'References' followed by '1.' \n",
    "    reference_pattern = r'\\bReferences\\s+1\\.\\s'\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(reference_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        # If 'References 1.' is found, remove everything from that point onward\n",
    "        return text[:match.start()].strip()\n",
    "    \n",
    "    # If no references section is found, return the original text\n",
    "    return text\n",
    "\n",
    "\n",
    "def detect_sentence_boundaries(text):\n",
    "    \"\"\"\n",
    "    Detect sentence boundaries using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    spacy_sentences = [sent.text for sent in doc.sents]\n",
    "    return ' '.join(spacy_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    \n",
    "    # Remove punctuation except for hyphens within words and apostrophes\n",
    "    text = re.sub(r'[^\\w\\s\\'-]|(?<!\\w)[-\\']|[-\\'](?!\\w)', '', text)\n",
    "    \n",
    "    # Split on whitespace and count non-empty words (including numbers)\n",
    "    words = [word for word in text.split() if word]\n",
    "    \n",
    "    return len(words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not isinstance(text, (str, bytes)):\n",
    "            raise TypeError('Expected a string or bytes-like object')\n",
    "\n",
    "        print(f\"Starting preprocessing...\")\n",
    "        initial_word_count = count_words(text)\n",
    "        print(f\"Initial word count: {initial_word_count}\")\n",
    "\n",
    "        # Step 1: Initial text cleaning\n",
    "        cleaned_text = reassemble_hyphenated_words(text)\n",
    "        print(f\"After reassemble_hyphenated_words: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_figures_tables(cleaned_text)\n",
    "        print(f\"After remove_figures_tables: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_citations(cleaned_text)\n",
    "        print(f\"After remove_citations: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_urls(cleaned_text)\n",
    "        print(f\"After remove_urls: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_emails(cleaned_text)\n",
    "        print(f\"After remove_emails: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_numerical_references(cleaned_text)\n",
    "        print(f\"After remove_numerical_references: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_headers(cleaned_text)\n",
    "        print(f\"After remove_headers: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_metadata(cleaned_text)\n",
    "        print(f\"After remove_metadata: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_copyright_info(cleaned_text)\n",
    "        print(f\"After remove_copyright_info: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_doi_and_journal_info(cleaned_text)\n",
    "        print(f\"After remove_doi_and_journal_info: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_artifacts(cleaned_text)\n",
    "        print(f\"After remove_artifacts: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        cleaned_text = ' '.join(cleaned_text.split())\n",
    "        print(f\"After removing extra whitespace: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = handle_special_characters(cleaned_text)\n",
    "        print(f\"After handle_special_characters: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = remove_references(cleaned_text)\n",
    "        print(f\"After remove_references: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        cleaned_text = detect_sentence_boundaries(cleaned_text)\n",
    "        print(f\"After detect_sentence_boundaries: {count_words(cleaned_text)} words\")\n",
    "\n",
    "        final_word_count = count_words(cleaned_text)\n",
    "        print(f\"Final preprocessed word count: {final_word_count}\")\n",
    "\n",
    "        if initial_word_count > 0:\n",
    "            percentage_retained = (final_word_count / initial_word_count) * 100\n",
    "            print(f\"Percentage of words retained after cleaning: {percentage_retained:.2f}%\")\n",
    "        else:\n",
    "            print(\"Original text contains no words; cannot calculate percentage.\")\n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "    except TypeError as te:\n",
    "        logging.error(f\"TypeError in preprocess_text: {te}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in preprocess_text: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract_and_main_text(cleaned_text):\n",
    "    try:\n",
    "        abstract_start_patterns = [\n",
    "            r'\\b(Abstract|Synopsis|Summary|Overview)\\b:?',\n",
    "            r'^\\s*(Background|Purpose|Objective|Aim)\\b:?',\n",
    "            r'^\\s*[A-Z][^.!?]+(?=[.!?])\\s'  # Captures the first sentence if it's capitalized\n",
    "        ]\n",
    "        abstract_end_patterns = [\n",
    "            r'\\b(Keywords|Key words|Index terms)\\b:?',\n",
    "            r'\\n{2,}\\s*(Introduction|Main Text|Methods|Methodology|Materials)\\b:?'\n",
    "        ]\n",
    "        \n",
    "        abstract_start = None\n",
    "        abstract_end = None\n",
    "\n",
    "        # Search for the start of the abstract\n",
    "        for pattern in abstract_start_patterns:\n",
    "            match = re.search(pattern, cleaned_text, re.IGNORECASE | re.MULTILINE)\n",
    "            if match:\n",
    "                abstract_start = match.start()\n",
    "                break\n",
    "\n",
    "        # If we cannot determine the start of the abstract, return empty abstract\n",
    "        if abstract_start is None:\n",
    "            return \"\", cleaned_text\n",
    "\n",
    "        # Search for the end of the abstract\n",
    "        for pattern in abstract_end_patterns:\n",
    "            match = re.search(pattern, cleaned_text[abstract_start:], re.IGNORECASE | re.MULTILINE)\n",
    "            if match:\n",
    "                abstract_end = abstract_start + match.start()\n",
    "                break\n",
    "\n",
    "        # If no clear end is found, use structural heuristics\n",
    "        if abstract_end is None:\n",
    "            paragraphs = cleaned_text[abstract_start:].split('\\n\\n')\n",
    "            if len(paragraphs) > 1:\n",
    "                abstract_end = abstract_start + len(paragraphs[0])\n",
    "            else:\n",
    "                # Fallback to a maximum word limit\n",
    "                words = cleaned_text[abstract_start:].split()\n",
    "                abstract_end = abstract_start + len(' '.join(words[:300]))\n",
    "\n",
    "        # Extract the abstract content\n",
    "        abstract = cleaned_text[abstract_start:abstract_end].strip()\n",
    "\n",
    "        # Ensure the abstract is between 50 and 500 words\n",
    "        abstract_words = abstract.split()\n",
    "        if len(abstract_words) < 50:\n",
    "            return \"\", cleaned_text  # Return empty abstract if too short\n",
    "        elif len(abstract_words) > 500:\n",
    "            abstract = ' '.join(abstract_words[:500])\n",
    "            abstract_end = abstract_start + len(abstract)\n",
    "\n",
    "        # Extract the main text starting from the end of the abstract\n",
    "        main_text = cleaned_text[abstract_end:].strip()\n",
    "\n",
    "        return abstract, main_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in extract_abstract_and_main_text: {e}\")\n",
    "        return \"\", cleaned_text  # Return empty abstract and full text as main_text in case of any error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(input_folder_path: str, output_folder_path: str, entities_path: str, batch_size: int = 10) -> None:\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    if not os.path.exists(entities_path):\n",
    "        os.makedirs(entities_path)\n",
    "\n",
    "    files = [f for f in os.listdir(input_folder_path) if f.endswith('.txt')]\n",
    "    total_files = len(files)\n",
    "\n",
    "    all_entities = []\n",
    "\n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch = files[i:i+batch_size]\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1} of {(total_files-1)//batch_size + 1}\")\n",
    "\n",
    "        for filename in tqdm(batch, desc=f\"Batch {i//batch_size + 1}\"):\n",
    "            input_file_path = os.path.join(input_folder_path, filename)\n",
    "            output_file_path = os.path.join(output_folder_path, f\"processed_{filename}\")\n",
    "\n",
    "            print(f\"\\nProcessing file: {filename}\")\n",
    "\n",
    "            # Corrected section in process_files\n",
    "            try:\n",
    "                with open(input_file_path, 'rb') as file:\n",
    "                    raw_data = file.read()\n",
    "                    result = chardet.detect(raw_data)\n",
    "                    detected_encoding = result['encoding']\n",
    "                    confidence = result['confidence']\n",
    "\n",
    "                    print(f\"Detected encoding: {detected_encoding} with confidence: {confidence}\")\n",
    "\n",
    "                # Fallback to utf-8 if encoding is not detected or confidence is low\n",
    "                encoding_to_use = detected_encoding if detected_encoding else 'utf-8'\n",
    "\n",
    "                with open(input_file_path, 'r', encoding=encoding_to_use) as file:\n",
    "                    original_text = file.read()\n",
    "\n",
    "                # Ensure the text is correctly read as a string\n",
    "                if not isinstance(original_text, str):\n",
    "                    raise ValueError(\"File content is not a valid string.\")\n",
    "\n",
    "                # Extract abstract and main text\n",
    "                abstract, main_text = extract_abstract_and_main_text(original_text)\n",
    "\n",
    "                # Preprocess the main text\n",
    "                main_text = preprocess_text(main_text)\n",
    "\n",
    "                # Extract entities\n",
    "                entities = extract_entities_with_links(main_text)\n",
    "                all_entities.extend(entities)\n",
    "\n",
    "                # Prepare output text\n",
    "                output_text = f\"Abstract:\\n{abstract}\\n\\nMain Text:\\n{main_text}\"\n",
    "\n",
    "                # Write the cleaned text and abstract to the output file\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(output_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {filename}: {str(e)}\")\n",
    "\n",
    "    # Save the extracted entities to a CSV file\n",
    "    entities_df = pd.DataFrame(all_entities)\n",
    "    entities_df.to_csv(os.path.join(entities_path, 'extracted_entities.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_frequent_terms(text, n=10, min_length=3):\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = re.findall(r'\\b[a-zA-Z]{' + str(min_length) + r',}\\b', text.lower())\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Return the n most common words with their counts\n",
    "    return word_freq.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 13287_2023_Article_3526.txt\n",
      "Detected encoding: utf-8 with confidence: 0.99\n",
      "Starting preprocessing...\n",
      "Initial word count: 6850\n",
      "After reassemble_hyphenated_words: 6826 words\n",
      "After remove_figures_tables: 6826 words\n",
      "After remove_citations: 6160 words\n",
      "After remove_urls: 6153 words\n",
      "After remove_emails: 6152 words\n",
      "After remove_numerical_references: 6152 words\n",
      "After remove_headers: 6152 words\n",
      "After remove_metadata: 6144 words\n",
      "After remove_copyright_info: 6118 words\n",
      "After remove_doi_and_journal_info: 6118 words\n",
      "After remove_artifacts: 6126 words\n",
      "After removing extra whitespace: 6126 words\n",
      "After handle_special_characters: 6126 words\n",
      "After remove_references: 4171 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After detect_sentence_boundaries: 4174 words\n",
      "Final preprocessed word count: 4174\n",
      "Percentage of words retained after cleaning: 60.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  17%|█▋        | 1/6 [00:01<00:09,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 12886_2023_Article_2772-compressed.txt\n",
      "Detected encoding: utf-8 with confidence: 0.99\n",
      "Starting preprocessing...\n",
      "Initial word count: 4410\n",
      "After reassemble_hyphenated_words: 4399 words\n",
      "After remove_figures_tables: 4399 words\n",
      "After remove_citations: 4202 words\n",
      "After remove_urls: 4201 words\n",
      "After remove_emails: 4200 words\n",
      "After remove_numerical_references: 4200 words\n",
      "After remove_headers: 4200 words\n",
      "After remove_metadata: 4192 words\n",
      "After remove_copyright_info: 4192 words\n",
      "After remove_doi_and_journal_info: 4192 words\n",
      "After remove_artifacts: 4193 words\n",
      "After removing extra whitespace: 4193 words\n",
      "After handle_special_characters: 4193 words\n",
      "After remove_references: 3752 words\n",
      "After detect_sentence_boundaries: 3754 words\n",
      "Final preprocessed word count: 3754\n",
      "Percentage of words retained after cleaning: 85.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  33%|███▎      | 2/6 [00:03<00:06,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 13023_2021_Article_2145.txt\n",
      "Detected encoding: utf-8 with confidence: 0.99\n",
      "Starting preprocessing...\n",
      "Initial word count: 6328\n",
      "After reassemble_hyphenated_words: 6307 words\n",
      "After remove_figures_tables: 6307 words\n",
      "After remove_citations: 5920 words\n",
      "After remove_urls: 5900 words\n",
      "After remove_emails: 5899 words\n",
      "After remove_numerical_references: 5899 words\n",
      "After remove_headers: 5899 words\n",
      "After remove_metadata: 5891 words\n",
      "After remove_copyright_info: 5865 words\n",
      "After remove_doi_and_journal_info: 5865 words\n",
      "After remove_artifacts: 5866 words\n",
      "After removing extra whitespace: 5866 words\n",
      "After handle_special_characters: 5866 words\n",
      "After remove_references: 4677 words\n",
      "After detect_sentence_boundaries: 4677 words\n",
      "Final preprocessed word count: 4677\n",
      "Percentage of words retained after cleaning: 73.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  67%|██████▋   | 4/6 [00:05<00:02,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 41433_2022_Article_2262.txt\n",
      "Detected encoding: utf-8 with confidence: 0.99\n",
      "Starting preprocessing...\n",
      "Initial word count: 455\n",
      "After reassemble_hyphenated_words: 453 words\n",
      "After remove_figures_tables: 453 words\n",
      "After remove_citations: 371 words\n",
      "After remove_urls: 370 words\n",
      "After remove_emails: 370 words\n",
      "After remove_numerical_references: 370 words\n",
      "After remove_headers: 370 words\n",
      "After remove_metadata: 369 words\n",
      "After remove_copyright_info: 369 words\n",
      "After remove_doi_and_journal_info: 369 words\n",
      "After remove_artifacts: 369 words\n",
      "After removing extra whitespace: 369 words\n",
      "After handle_special_characters: 369 words\n",
      "After remove_references: 369 words\n",
      "After detect_sentence_boundaries: 370 words\n",
      "Final preprocessed word count: 370\n",
      "Percentage of words retained after cleaning: 81.32%\n",
      "\n",
      "Processing file: 41436_2020_Article_759.txt\n",
      "Detected encoding: utf-8 with confidence: 0.99\n",
      "Starting preprocessing...\n",
      "Initial word count: 3511\n",
      "After reassemble_hyphenated_words: 3499 words\n",
      "After remove_figures_tables: 3499 words\n",
      "After remove_citations: 3127 words\n",
      "After remove_urls: 3123 words\n",
      "After remove_emails: 3123 words\n",
      "After remove_numerical_references: 3123 words\n",
      "After remove_headers: 3123 words\n",
      "After remove_metadata: 3107 words\n",
      "After remove_copyright_info: 3092 words\n",
      "After remove_doi_and_journal_info: 3092 words\n",
      "After remove_artifacts: 3096 words\n",
      "After removing extra whitespace: 3096 words\n",
      "After handle_special_characters: 3096 words\n",
      "After remove_references: 2170 words\n",
      "After detect_sentence_boundaries: 2175 words\n",
      "Final preprocessed word count: 2175\n",
      "Percentage of words retained after cleaning: 61.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  83%|████████▎ | 5/6 [00:06<00:01,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 13023_2023_Article_2798.txt\n",
      "Detected encoding: utf-8 with confidence: 0.99\n",
      "Starting preprocessing...\n",
      "Initial word count: 8723\n",
      "After reassemble_hyphenated_words: 8695 words\n",
      "After remove_figures_tables: 8693 words\n",
      "After remove_citations: 8042 words\n",
      "After remove_urls: 8034 words\n",
      "After remove_emails: 8034 words\n",
      "After remove_numerical_references: 8034 words\n",
      "After remove_headers: 7963 words\n",
      "After remove_metadata: 7955 words\n",
      "After remove_copyright_info: 7955 words\n",
      "After remove_doi_and_journal_info: 7955 words\n",
      "After remove_artifacts: 7957 words\n",
      "After removing extra whitespace: 7957 words\n",
      "After handle_special_characters: 7957 words\n",
      "After remove_references: 6251 words\n",
      "After detect_sentence_boundaries: 6255 words\n",
      "Final preprocessed word count: 6255\n",
      "Percentage of words retained after cleaning: 71.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 6/6 [00:08<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and entity extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # input_folder = \"/mnt/data/skanda/MSc_IRD_LLM/data/txt_data\"\n",
    "    input_folder = \"/mnt/data/skanda/MSc_IRD_LLM/data/data_text_sample\"\n",
    "    output_folder = \"/mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed\"\n",
    "    entities_path = \"/mnt/data/skanda/MSc_IRD_LLM/data/Entities/\"\n",
    "    process_files(input_folder, output_folder, entities_path, batch_size=10)\n",
    "    print(\"Preprocessing and entity extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: processed_13287_2023_Article_3526.txt\n",
      "Successfully processed and saved: processed_13287_2023_Article_3526.txt\n",
      "Processing file: processed_13023_2023_Article_2798.txt\n",
      "Successfully processed and saved: processed_13023_2023_Article_2798.txt\n",
      "Processing file: processed_41436_2020_Article_759.txt\n",
      "Successfully processed and saved: processed_41436_2020_Article_759.txt\n",
      "Processing file: processed_13023_2021_Article_2145.txt\n",
      "Successfully processed and saved: processed_13023_2021_Article_2145.txt\n",
      "Processing file: processed_41433_2022_Article_2262.txt\n",
      "Successfully processed and saved: processed_41433_2022_Article_2262.txt\n",
      "Processing file: processed_12886_2023_Article_2772-compressed.txt\n",
      "Successfully processed and saved: processed_12886_2023_Article_2772-compressed.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def separate_abstract_and_main_text(text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Separates the abstract and main text from the given text.\n",
    "    \n",
    "    :param text: The input text containing both abstract and main text.\n",
    "    :return: A tuple containing the abstract and main text as separate strings.\n",
    "    \"\"\"\n",
    "    abstract_marker = \"Abstract:\"\n",
    "    main_text_marker = \"Main Text:\"\n",
    "    \n",
    "    # Find the start of the abstract\n",
    "    abstract_start = text.find(abstract_marker)\n",
    "    # Find the start of the main text\n",
    "    main_text_start = text.find(main_text_marker)\n",
    "    \n",
    "    # Extract abstract and main text\n",
    "    if abstract_start != -1 and main_text_start != -1:\n",
    "        abstract = text[abstract_start + len(abstract_marker):main_text_start].strip()\n",
    "        main_text = text[main_text_start + len(main_text_marker):].strip()\n",
    "    else:\n",
    "        raise ValueError(\"The text does not contain the required markers for Abstract and Main Text.\")\n",
    "    \n",
    "    return abstract, main_text\n",
    "\n",
    "def process_preprocessed_files(input_folder_path: str, output_folder_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Processes all .txt files in the input folder, separates abstracts and main texts,\n",
    "    and saves them into the output folder.\n",
    "    \n",
    "    :param input_folder_path: The path to the input folder containing .txt files.\n",
    "    :param output_folder_path: The path to the output folder where separated files will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "    \n",
    "    # Get a list of all .txt files in the input directory\n",
    "    files = [f for f in os.listdir(input_folder_path) if f.endswith('.txt')]\n",
    "    \n",
    "    for filename in files:\n",
    "        input_file_path = os.path.join(input_folder_path, filename)\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the content of the file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "            \n",
    "            # Separate abstract and main text\n",
    "            abstract, main_text = separate_abstract_and_main_text(file_content)\n",
    "            \n",
    "            # Define paths for the abstract and main text files\n",
    "            abstract_output_path = os.path.join(output_folder_path, f\"{filename[:-4]}_abstract.txt\")\n",
    "            main_text_output_path = os.path.join(output_folder_path, f\"{filename[:-4]}_main_text.txt\")\n",
    "            \n",
    "            # Save the abstract\n",
    "            with open(abstract_output_path, 'w', encoding='utf-8') as abstract_file:\n",
    "                abstract_file.write(abstract)\n",
    "            \n",
    "            # Save the main text\n",
    "            with open(main_text_output_path, 'w', encoding='utf-8') as main_text_file:\n",
    "                main_text_file.write(main_text)\n",
    "            \n",
    "            print(f\"Successfully processed and saved: {filename}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {str(e)}\")\n",
    "\n",
    "# Example usage:\n",
    "input_folder = '/mnt/data/skanda/MSc_IRD_LLM/data/data_preprocessed'\n",
    "output_folder = '/mnt/data/skanda/MSc_IRD_LLM/data/data_separated'\n",
    "\n",
    "process_preprocessed_files(input_folder, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
